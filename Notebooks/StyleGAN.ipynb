{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mWQmOeNZ5fk"
      },
      "source": [
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfqKpo9beWMu"
      },
      "source": [
        "import torch ## Deep Learning Framework\n",
        "from torch import nn ## Neural Nets\n",
        "\n",
        "from torchsummary import summary ## To get summary of model\n",
        "import os\n",
        "from torch.utils.data import Dataset , DataLoader ## Custom dataset , dataloader\n",
        "from torchvision import transforms ## transformation for image\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glJMR9U5owXX"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') ## Device cuda or cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTKxWxiv-coD"
      },
      "source": [
        "def show_tensor_images(image_tensor, num_images=2, size=(3 , 256 , 256)):\n",
        "  image_shifted = image_tensor\n",
        "  image_unflat = image_shifted.detach().cpu().view(-1, *size)\n",
        "  image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "  plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB0phNcsegHS"
      },
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_dropout = False , \n",
        "                 n_slope = 0.2 , \n",
        "                 p_dropout = 0.5):\n",
        "        super(Linear , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        self.linear = nn.Linear(in_channels , out_channels)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.BatchNorm1d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(n_slope)\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(p_dropout)\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.linear(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1X623wnmFIT"
      },
      "source": [
        "## Linear test\n",
        "linear = Linear(512 , 256).to(device)\n",
        "x = torch.randn(2 , 512).to(device)\n",
        "out = linear(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq5KBGnMo5mQ"
      },
      "source": [
        "class Mapping_Network(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels = 512, \n",
        "                 out_channels = 512 , \n",
        "                 hidden_dim = 32):\n",
        "        super(Mapping_Network , self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            Linear(in_channels , hidden_dim) ,\n",
        "            Linear(hidden_dim  , hidden_dim * 2) , \n",
        "            Linear(hidden_dim * 2, hidden_dim * 4) , \n",
        "            Linear(hidden_dim * 4 , hidden_dim * 8) , \n",
        "            Linear(hidden_dim * 8 , hidden_dim * 16) , \n",
        "            Linear(hidden_dim * 16 , hidden_dim * 32) , \n",
        "            Linear(hidden_dim * 32 , hidden_dim * 64) , \n",
        "            Linear(hidden_dim * 64 , out_channels)\n",
        "        )\n",
        "    def forward(self , x):\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40lwvqvLp7SP"
      },
      "source": [
        "mapping_network = Mapping_Network().to(device)\n",
        "x = torch.randn(2 , 512).to(device)\n",
        "out = mapping_network(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlSiT2DYqMk0"
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels, \n",
        "                 out_channels , \n",
        "                 kernel_size = (3 , 3) , \n",
        "                 stride = (1 , 1) , \n",
        "                 padding = 1 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_pool = True , \n",
        "                 n_slope = 0.2):\n",
        "        super(Conv , self).__init__()\n",
        "\n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_pool = use_pool\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels ,\n",
        "                              out_channels , \n",
        "                              kernel_size , \n",
        "                              stride , \n",
        "                              padding)\n",
        "        if self.use_norm:\n",
        "            self.norm = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(n_slope)\n",
        "        if self.use_pool:\n",
        "            self.pool = nn.MaxPool2d(kernel_size=(2 , 2) , stride=(2 , 2))\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.conv(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_pool:\n",
        "            x = self.pool(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or8oh4MHqxLA"
      },
      "source": [
        "conv = Conv(3 , 512).to(device)\n",
        "summary(conv , (3 , 512 , 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeUDQ81ZsXVl"
      },
      "source": [
        "class ConvT(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels, \n",
        "                 out_channels , \n",
        "                 kernel_size = (2 , 2) , \n",
        "                 stride = (2 ,2) , \n",
        "                 padding = 0 , \n",
        "                 use_norm = True , \n",
        "                 use_activation = True , \n",
        "                 use_second_convT = False , \n",
        "                 n_slope = 0.2):\n",
        "        super(ConvT , self).__init__()\n",
        "        \n",
        "        self.use_norm = use_norm\n",
        "        self.use_activation = use_activation\n",
        "        self.use_second_convT = use_second_convT\n",
        "\n",
        "        self.convT1 = nn.ConvTranspose2d(in_channels , out_channels , kernel_size , stride , padding)\n",
        "        if self.use_norm:\n",
        "            self.norm1 = nn.InstanceNorm2d(out_channels)\n",
        "        if self.use_activation:\n",
        "            self.activation = nn.LeakyReLU(n_slope)\n",
        "        if self.use_second_convT:\n",
        "            self.convT2 = nn.ConvTranspose2d(out_channels , out_channels , kernel_size , stride , padding)\n",
        "            if self.use_norm:\n",
        "                self.norm2 = nn.InstanceNorm2d(out_channels * 2)\n",
        "    \n",
        "    def forward(self , x):\n",
        "        x = self.convT1(x)\n",
        "        if self.use_norm:\n",
        "            x = self.norm1(x)\n",
        "        if self.use_activation:\n",
        "            x = self.activation(x)\n",
        "        if self.use_second_convT:\n",
        "            x = self.convT2(x)\n",
        "            if self.use_norm:\n",
        "                x = self.norm2(x)\n",
        "            if self.use_activation:\n",
        "                x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WitxAWVf8Zt0"
      },
      "source": [
        "convT = ConvT(512 , 256 , use_second_convT=True).to(device)\n",
        "summary(convT , (512 , 2 , 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y17XnWcz8aAr"
      },
      "source": [
        "class AdaIN(nn.Module):\n",
        "    def __init__(self , in_channels):\n",
        "        super(AdaIN , self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.norm = nn.InstanceNorm2d(in_channels)\n",
        "\n",
        "    def forward(self , x , y):\n",
        "        out = torch.mean(y) * self.norm(x) + torch.var(y)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXWc_0EnNOkq"
      },
      "source": [
        "class A_Block(nn.Module):\n",
        "    ## [512 , 256 , 128 , 64 , 32 , 16 , 8 , 4]\n",
        "    def __init__(self ,\n",
        "                 in_channels , \n",
        "                 out_channels , \n",
        "                 first_layer = False):\n",
        "        super(A_Block , self).__init__()\n",
        "        \n",
        "        if first_layer:\n",
        "            self.convT = ConvT(in_channels , out_channels , use_second_convT=True)\n",
        "        else:\n",
        "            self.convT = ConvT(in_channels , out_channels)\n",
        "        \n",
        "    def forward(self , x):\n",
        "        x = self.convT(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkJ0yHjYQfQ-"
      },
      "source": [
        "class A_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(A_Net , self).__init__()\n",
        "\n",
        "        self.network1 = nn.ModuleList([\n",
        "                                       A_Block(512 , 512 , first_layer=True) , \n",
        "                                       A_Block(256 , 256) , \n",
        "                                       A_Block(128 , 128) , \n",
        "                                       A_Block(64 , 64) , \n",
        "                                       A_Block(32 , 32) , \n",
        "                                       A_Block(16 , 16) , \n",
        "                                       A_Block(8 , 8) , \n",
        "                                       A_Block(4 , 4)\n",
        "                                       \n",
        "        ])\n",
        "        self.network2 = nn.ModuleList([\n",
        "                                       Conv(512 , 256 , use_pool=False) , \n",
        "                                       Conv(256 , 128 , use_pool=False) , \n",
        "                                       Conv(128 , 64 , use_pool=False) , \n",
        "                                       Conv(64 , 32 , use_pool=False) , \n",
        "                                       Conv(32 , 16 , use_pool=False) , \n",
        "                                       Conv(16 , 8 , use_pool=False) , \n",
        "                                       Conv(8 , 4 , use_pool=False)\n",
        "        ])\n",
        "\n",
        "    def forward(self , x , layer):\n",
        "        if layer == 0:\n",
        "            x1 = self.network1[layer](x)\n",
        "            x2 = self.network2[layer](x1)\n",
        "            return x1 , x2\n",
        "        else:\n",
        "            for l in range(0 , layer+1):\n",
        "                x = self.network1[l](x)\n",
        "                if l == layer:\n",
        "                    x1 = x\n",
        "                x = self.network2[l](x)\n",
        "                if l == layer:\n",
        "                    x2 = x\n",
        "            return x1 , x2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YiUjpNLSmlo"
      },
      "source": [
        "a_ = A_Net().to(device)\n",
        "x = torch.randn(2 , 512 , 1 , 1).to(device)\n",
        "out1 , out2 = a_(x , 6)\n",
        "print(out1.shape , out2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPKbA5T9fujo"
      },
      "source": [
        "class B_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(B_Net , self).__init__()\n",
        "\n",
        "        self.conv = nn.ModuleList([\n",
        "                                   Conv(512 , 512 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(256 , 256 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(128 , 128 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(64 , 64 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(32 , 32 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(16 , 16 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(8 , 8 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                   Conv(4 , 4 , kernel_size=1 , stride=1 , padding=0 , use_pool=False)\n",
        "        ])\n",
        "\n",
        "        self.conv_ = nn.ModuleList([\n",
        "                                    Conv(512 , 256 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(256 , 128 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(128 , 64 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(64 , 32 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(32 , 16 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(16 , 8 , kernel_size=1 , stride=1 , padding=0 , use_pool=False),\n",
        "                                    Conv(8 , 4 , kernel_size=1 , stride=1 , padding=0 , use_pool=False)\n",
        "        ])\n",
        "\n",
        "    def forward(self , x , layer):\n",
        "        if layer == 0:\n",
        "            x1 = self.conv[layer](x)\n",
        "            x2 = self.conv_[layer](x1)\n",
        "            return x1 , x2\n",
        "        else:\n",
        "            for l in range(0 , layer+1):\n",
        "                x = self.conv[l](x)\n",
        "                if l == layer:\n",
        "                    x1 = x\n",
        "                x = self.conv_[l](x)\n",
        "                if l == layer:\n",
        "                    x2 = x\n",
        "            return x1 , x2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MzX3nehbhE"
      },
      "source": [
        "b_ = B_Net().to(device)\n",
        "x = torch.randn(2 , 512 , 1 , 1).to(device)\n",
        "out1 , out2 = b_(x , 6)\n",
        "print(out1.shape , out2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7THRBE8b_B8w"
      },
      "source": [
        "class Style_Block(nn.Module):\n",
        "    def __init__(self , \n",
        "                 in_channels , \n",
        "                 out_channels ):\n",
        "        super(Style_Block , self).__init__()\n",
        "\n",
        "\n",
        "        self.adain = AdaIN(in_channels)\n",
        "\n",
        "        self.conv1 = Conv(in_channels , in_channels , use_pool=False)\n",
        "        self.conv2 = Conv(in_channels , out_channels , use_pool=False)\n",
        "\n",
        "        self.a_net = A_Net()\n",
        "        self.b_net = B_Net()\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def forward(self , x , w , noise , layer):\n",
        "        b1 , b2 = self.b_net(noise , layer)\n",
        "        a1 , a2 = self.a_net(w , layer)\n",
        "        if layer == 0:\n",
        "            x = x + b1\n",
        "            x = self.adain(x , a1)\n",
        "            x = self.conv2(x)\n",
        "            x = x + b2\n",
        "            x = self.adain(x , a2)\n",
        "            return x\n",
        "        else:\n",
        "            x = self.upsample(x)\n",
        "            x = self.conv1(x)\n",
        "            x = x + b1\n",
        "            x = self.adain(x , a1)\n",
        "            x = self.conv2(x)\n",
        "            x = x + b2\n",
        "            x = self.adain(x , a2)\n",
        "            return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNyPSgGz_TN0"
      },
      "source": [
        "styled_conv = Style_Block(256 , 128).to(device)\n",
        "x = torch.randn(2 , 256 , 4 , 4).to(device)\n",
        "w = torch.randn(2 , 512 , 1 , 1).to(device)\n",
        "noise = torch.randn(2 , 512 , 1 , 1).to(device)\n",
        "x = styled_conv(x , w , noise , 1)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlYMPs_I_TkT"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self , \n",
        "                 batch_size , \n",
        "                 device = device):\n",
        "        super(Generator , self).__init__()\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        self.network = nn.ModuleList([\n",
        "                                      Style_Block(512 , 256) , \n",
        "                                      Style_Block(256 , 128) , \n",
        "                                      Style_Block(128 , 64) , \n",
        "                                      Style_Block(64 ,32) , \n",
        "                                      Style_Block(32 , 16) , \n",
        "                                      Style_Block(16 , 8) , \n",
        "                                      Style_Block(8 , 4)\n",
        "        ])\n",
        "        \n",
        "        self.to_rgb = nn.ModuleList([\n",
        "                                     Conv(256 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(128 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(64 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(32 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(16 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(8 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                     Conv(4 , 3 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False)\n",
        "        ])\n",
        "\n",
        "\n",
        "    def forward(self , x , layer):\n",
        "        w = torch.randn(self.batch_size , 512 , 1 , 1).to(self.device)\n",
        "        noise = torch.randn(self.batch_size , 512 , 1 , 1).to(self.device)\n",
        "        if layer == 0:\n",
        "            x = torch.randn((self.batch_size , 512 , 4 , 4)).to(self.device)\n",
        "            x = self.network[layer](x ,w , noise , layer)\n",
        "            x_ = self.to_rgb[layer](x)\n",
        "            return x , x_\n",
        "        else:\n",
        "            x = self.network[layer](x ,w , noise , layer)\n",
        "            x_ = self.to_rgb[layer](x)\n",
        "            return x , x_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78fO2kx0mQH4"
      },
      "source": [
        "generator = Generator(2).to(device)\n",
        "x = torch.randn(2 , 512 , 4 , 4).to(device)\n",
        "out , out_rbg = generator(x , 0)\n",
        "out.shape , out_rbg.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je25I82ZmbAV"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator , self).__init__()\n",
        "\n",
        "        self.from_rgb = nn.ModuleList([\n",
        "                                       Conv(3 , 256 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 128 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 64 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 32 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 16 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 8 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False),\n",
        "                                       Conv(3 , 4 , kernel_size=(1 , 1) , stride=(1 , 1) , padding=0 , use_pool=False)\n",
        "        ])\n",
        "\n",
        "        self.network = nn.ModuleList([ \n",
        "                                      Conv(256 , 128) , \n",
        "                                      Conv(128 , 64) , \n",
        "                                      Conv(64 , 32) , \n",
        "                                      Conv(32 , 16) , \n",
        "                                      Conv(16 , 8) , \n",
        "                                      Conv(8 , 4)\n",
        "        ])\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Sequential(\n",
        "            Linear(64 , 32) , \n",
        "            Linear(32 , 16) , \n",
        "            Linear(16, 8) , \n",
        "            Linear(8 , 4) , \n",
        "            Linear(4 , 1 , use_activation=False , use_norm=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self , x , layer):\n",
        "        x = self.from_rgb[layer](x)\n",
        "        for l in range(layer , len(self.network)):\n",
        "            x = self.network[l](x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkqkLb0TtZIT"
      },
      "source": [
        "discriminator = Discriminator().to(device)\n",
        "x = torch.randn(2 , 3 , 8 , 8).to(device)\n",
        "out = discriminator(x , 5)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "044LAqohtdJ3"
      },
      "source": [
        "def test():\n",
        "    gen = Generator(2).to(device)\n",
        "    disc = Discriminator().to(device)\n",
        "    channels = 512\n",
        "    shape_ = 4\n",
        "    for layer in range(0 , 7):\n",
        "        x = torch.randn(2 , channels , shape_ , shape_).to(device)\n",
        "        print(f'x {x.shape} , layer {layer}')\n",
        "        x , x_rgb = gen(x , layer)\n",
        "        print(torch.max(x) , torch.min(x))\n",
        "        disc_pred = disc(x_rgb , 6 - layer)\n",
        "        channels = channels // 2\n",
        "        if layer == 0:\n",
        "            shape_ = shape_\n",
        "        else:\n",
        "            shape_ = shape_ * 2\n",
        "        print(f'x {x.shape} , disc pred {disc_pred.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dvOdm2j99V0"
      },
      "source": [
        "def test():\n",
        "    gen = Generator(2).to(device)\n",
        "    disc = Discriminator().to(device)\n",
        "    channels = 512\n",
        "    shape_ = 4\n",
        "    x = torch.randn(2 , 512 , 4 , 4).to(device)\n",
        "    for layer in range(0 , 7):\n",
        "        print(f'x {x.shape} , layer {layer}')\n",
        "        x , x_rgb = gen(x , layer)\n",
        "        disc_pred = disc(x_rgb , 6 - layer)\n",
        "        print(f'x {x.shape} , x_rgb {x_rgb.shape} , disc pred {disc_pred.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqT4SEy-9358"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64TTm3-M94ij"
      },
      "source": [
        "def resize_on_the_fly(img , layer):\n",
        "    sizes = [4 , 8 , 16 , 32 , 64 , 128 , 256]\n",
        "    img = transforms.functional.resize(img , [sizes[layer] , sizes[layer]])\n",
        "    return img "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy-QKbfTb2KB"
      },
      "source": [
        "x = torch.randn(2 , 3 , 512 , 512).to(device)\n",
        "x = resize_on_the_fly(x , 1)\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4H3AwU878f2"
      },
      "source": [
        "def get_dataset_mapped(root_dir = '/content/drive/MyDrive/celeb_hq/celeba_hq/train/'):\n",
        "    img_paths = []\n",
        "    for data in os.listdir(root_dir):\n",
        "        for img in os.listdir(os.path.join(root_dir , data)):\n",
        "            img_paths.append(os.path.join(root_dir , data , img))\n",
        "    return img_paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "754bs8KN8Rq7"
      },
      "source": [
        "img_paths = get_dataset_mapped()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJbZGpEQdEbi"
      },
      "source": [
        "class Dataset_(Dataset):\n",
        "    def __init__(self , \n",
        "                transforms = None, \n",
        "                img_paths = img_paths):\n",
        "        super(Dataset_ , self).__init__()\n",
        "        self.img_paths = img_paths\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self , idx):\n",
        "        img = self.img_paths[idx]\n",
        "        img = np.asarray(plt.imread(img))\n",
        "        img_tensor = torch.from_numpy(img).permute(2 , 0 , 1)\n",
        "        if self.transforms:\n",
        "            img_tensor = self.transforms(img_tensor)\n",
        "        return img_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsMyl-uj9UGU"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToPILImage() , \n",
        "                                transforms.Resize((256 , 256)) , \n",
        "                                transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Eriglw627V"
      },
      "source": [
        "dataset = Dataset_(transforms=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cexftiP_XCF"
      },
      "source": [
        "adv_criterion = nn.BCEWithLogitsLoss()\n",
        "recon_criterion = nn.L1Loss()\n",
        "lambda_recon = 200\n",
        "betas = (0.5 , 0.999)\n",
        "\n",
        "n_layers = 6\n",
        "n_epochs = 1\n",
        "in_channels = 3\n",
        "out_channels = 3\n",
        "display_step = 100\n",
        "batch_size = 2\n",
        "lr = 0.0002\n",
        "target_shape = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4Dxc-b165nT"
      },
      "source": [
        "dataloader = DataLoader(dataset , batch_size=batch_size , shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyeujwMy-JqI"
      },
      "source": [
        "for x in dataloader:\n",
        "    show_tensor_images(x)\n",
        "    print(torch.max(x) , torch.min(x))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8RlLrGC-nMY"
      },
      "source": [
        "generator = Generator(batch_size).to(device)\n",
        "opt_generator = torch.optim.Adam(generator.parameters() , lr=lr , betas = betas)\n",
        "discriminator = Discriminator().to(device)\n",
        "opt_discriminator = torch.optim.Adam(discriminator.parameters() , lr=lr , betas = betas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzpLLbnW_q8I"
      },
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        torch.nn.init.normal_(m.weight, 0.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVMbz1ZG_vRv"
      },
      "source": [
        "generator = generator.apply(weights_init)\n",
        "discriminator = discriminator.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2sYGfmV_1SH"
      },
      "source": [
        "def get_generator_loss(fake , \n",
        "                       real , \n",
        "                       layer , \n",
        "                       generator = generator , \n",
        "                       discriminator = discriminator , \n",
        "                       adv_criterion = adv_criterion , \n",
        "                       recon_criterion = recon_criterion , \n",
        "                       lambda_recon = lambda_recon):\n",
        "    disc_pred = discriminator(fake , 6 - layer)\n",
        "    disc_loss = adv_criterion(disc_pred , torch.zeros_like(disc_pred))\n",
        "    generator_loss = recon_criterion(fake , real)\n",
        "    loss = disc_loss + lambda_recon * generator_loss\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb1XdWr7BtJy"
      },
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EibFLkQTAs1D"
      },
      "source": [
        "def train():\n",
        "    mean_generator_loss = 0\n",
        "    mean_discriminator_loss = 0\n",
        "    cur_step = 0\n",
        "    best_loss = 0\n",
        "    for layer in range(n_layers):\n",
        "        for epoch in range(n_epochs):\n",
        "            for img in tqdm(dataloader):\n",
        "                x = torch.randn(batch_size , 512 , 4 , 4).to(device)\n",
        "                x1 = torch.randn_like(x)\n",
        "                for l in range(layer + 1):\n",
        "                    img = img.to(device)\n",
        "                    img = resize_on_the_fly(img , l)\n",
        "\n",
        "                    opt_generator.zero_grad()\n",
        "                    if l == 0:\n",
        "                        x_next , fake_img = generator(x , l)\n",
        "                        x_next = torch.tensor(x_next , requires_grad=False)\n",
        "                    else:\n",
        "                        if l == layer:\n",
        "                            x_next , fake_img = generator(x_next , l)\n",
        "                            x_next = torch.tensor(x_next , requires_grad=False)\n",
        "                        else:\n",
        "                            with torch.no_grad():\n",
        "                                x_next , fake_img = generator(x_next , l)\n",
        "                                x_next = torch.tensor(x_next , requires_grad=False)\n",
        "                    generator_loss = get_generator_loss(fake_img , img , l)\n",
        "                    generator_loss.backward(retain_graph=True)\n",
        "                    opt_generator.step()\n",
        "\n",
        "                    opt_discriminator.zero_grad()\n",
        "                    with torch.no_grad():\n",
        "                        if l == 0:\n",
        "                            img_ , fake_img_ = generator(x1 , l)\n",
        "                        else:\n",
        "                            img_ , fake_img_ = generator(img_ , l) \n",
        "                            img_ = torch.tensor(img_ , requires_grad=False)\n",
        "                    disc_fake_pred = discriminator(fake_img_ , 6 - l)\n",
        "                    disc_real_pred = discriminator(img , 6 - l)\n",
        "                    disc_fake_loss = adv_criterion(disc_fake_pred , torch.zeros_like(disc_fake_pred))\n",
        "                    disc_real_loss = adv_criterion(disc_real_pred , torch.ones_like(disc_real_pred))\n",
        "                    discriminator_loss = (disc_fake_loss + disc_real_loss) /2.0\n",
        "                    discriminator_loss.backward(retain_graph=True)\n",
        "                    opt_discriminator.step()\n",
        "\n",
        "                    mean_discriminator_loss += discriminator_loss.item() / display_step\n",
        "                    mean_generator_loss += generator_loss.item() / display_step\n",
        "\n",
        "                    if best_loss < mean_generator_loss:\n",
        "                        print('Saving ....')\n",
        "                        torch.save(generator.state_dict() , '/content/drive/MyDrive/StyleGAN_Weights/Generator.pt')\n",
        "                        torch.save(discriminator.state_dict() , '/content/drive/MyDrive/StyleGAN_Weights/Discriminator.pt')\n",
        "                        best_loss = mean_generator_loss\n",
        "                \n",
        "                    if cur_step % display_step == 0:\n",
        "                        if cur_step > 0:\n",
        "                            print(f\"Epoch {epoch}: layer{l} Step {cur_step}: Generator loss: {mean_generator_loss}, Discriminator loss: {mean_discriminator_loss}\")\n",
        "                        else:\n",
        "                            print(\"Pretrained initial state\")\n",
        "                        print('real image')\n",
        "                        show_tensor_images(img , size=img.shape[1:])\n",
        "                        print('Generated image')\n",
        "                        show_tensor_images(fake_img_ , size=fake_img_.shape[1:])\n",
        "                        mean_generator_loss = 0\n",
        "                        mean_discriminator_loss = 0\n",
        "                    cur_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRotNVmRA3ZI"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOUTwMgI3g-C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}